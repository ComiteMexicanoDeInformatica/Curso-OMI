# Complejidad

En computación existen diferentes tipos de algoritmos. Algunos de ellos son más rápidos que otros, y la manera de clasificar qué tanto tarda un algoritmo en ejecutarse es mediante su complejidad. Así pues, podemos tomar por el momento una definición vaga de complejidad, la cual es que un algoritmo tarda más que otro si su complejidad es mayor. Tomemos una definición un poco mas rigurosa. Definamos la complejidad de un algoritmo como la cantidad de operaciones que realiza el algoritmo, de modo que, en efecto, si un algoritmo realiza más operaciones que otro, entonces es más "lento". Con esto, demos una definición un poco más rigurosa.

Definamos $f(A)$ como la cantidad de operaciones que realiza el algoritmo $A$. Denotemos como $O(f(A))$ a la complejidad de un algoritmo $A$ que realiza $f(A)$ operaciones. Es decir, la letra $O$ nos sirve para denotar que de lo que estamos hablando es una complejidad.

Ahora nos hace falta definir qué significa cantidad de operaciones de una manera más rigurosa. Para esto, daremos unos cuantos ejemplos. Supongamos que tenemos un algoritmo $A$ que solo imprime un mensaje en pantalla. De modo que solo hace una operación, de donde se sigue que $f(A)=1$, lo que implica que la complejidad de $A$ es $O(1)$. Ahora, supongamos que tenemos un algoritmo $A$ que lee un arreglo de $n$ números, los suma e imprime el resultado. Entonces, leer $n$ números nos toma $n$ operaciones, una por cada número, e imprimir la suma nos toma una sola operación. De modo que $f(A)=n+1$ de donde se sigue que la complejidad de $A$ es $O(n+1)$. Demos un ejemplo más. Supongamos que tenemos un algoritmo $A$ que  lee una matriz de tamaño $n \cdot m$. Entonces, tenemos que por cada una de las $n$ filas tenemos que leer $m$ entradas, tomandonos una operación leer cada entrada. De modo que por el principio de conteo, tendremos $n \cdot m$ operaciones. Lo que nos dice que $f(A)=n \cdot m$, de donde se sigue que la complejidad de $A$ es $O(n \cdot m)$.

Ya que tenemos claro que representa la complejidad, podemos entonces introducir las propiedades de la notación $O$.

Cuando se escribe la complejidad de un algoritmo $A$ en términos de $O$, solo nos interesa escribir la cantidad de operaciones que realiza $A$ en términos de las variables que se usan en el algoritmo $A$. Es decir, las constantes no nos interesan. Así pues, si nuestro algoritmo realiza $n+1$ operaciones, dicho de otra forma, $f(A)=n+1$, entonces podemos escribir la complejidad de $A$ como $O(n)$. Esto pues $1$ es una constante, es decir, nunca cambia su valor. Mientras que $n$ si puede variar su valor. De ahí los nombres de variable y constante. Uno varía y el otro permanece constante.

Por lo tanto, cuando hablamos de la complejidad del algoritmo, solo nos interesan las variables. Demos unos cuantos ejemplos. Supongamos que tenemos un algoritmo $A$ con $f(A)=n^2+100000$. En este caso, $n^2$ no es constante, pues depende de $n$. Mientras que 100000 si es constante. De modo que la complejidad de $A$ se reduce a $O(n^2)$. Ahora supongamos que nuestro algoritmo $A$ realiza $f(A) = 100 \cdot n + n + 5$ operaciones. Entonces, se tiene que $5$ y $100$ son constantes, por lo que la complejidad de $A$ es $O(n + n) = O(2n)$. Pero ya que $2$ es una constante, entonces lo podemos reescribir como $O(n)$. Por lo tanto, el algoritmo $A$ tiene complejidad $O(n)$.

Ahora, cuando tenemos una complejidad de la forma $O(n^2 + n)$, podemos también aplicar la propiedad que acabamos de definir. Esto pues se tiene que $n \leq n^2$ para cualquier $n$ entera. Así pues, podemos notar que $n^2+n \leq n^2 + n^2 = 2n^2$. Entonces, podemos reescribir nuestra complejidad como $O(2n^2)$, y por lo anterior, esto es igual a $O(n^2)$.

Ya que tenemos una buena idea intuitiva de complejidad, podemos dar ejemplos de para que sirve esta herramienta. Pero antes, hay que definir que significa que un programa "corre en tiempo".

A una máquina le toma alrededor de $1$ segundo hacer $70,000,000$ operaciones. Con esta información, es más fácil ver como podemos usar la complejidad de una forma útil. Por ejemplo, si nos dan como tiempo límite por caso $1$ segundo, y con las herramientas anteriores deducimos que nuestro algoritmo es $O(n^3)$ con $n \leq 1000$, entonces se tiene que $n^3 = 1,000,000,000 > 70,000,000$, por lo que nuestro algoritmo no correría en $1$ segundo. Esta es la definición de que un algoritmo no "entra en tiempo".

De modo que, con la complejidad podemos saber cuando un algoritmo va a "entrar" en tiempo antes de implementarlo, y cuando no.
